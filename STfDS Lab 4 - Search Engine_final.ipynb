{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine\n",
    "\n",
    "This lab is about starting from scratch, and hence having to structure the code yourself. A specification for the program you are to implement is given below - do pay attention as misunderstandings may cost you marks! Being precise is a necessary skill for a programmer. To give an executive summary, you are to code a search engine for recipes. A data set has been provided. The search engine is to be pretty basic, returning all recipes that contain all of the provided keywords. However, the user can choose from a number of orderings depending on their food preferences, which you need to support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking and submission\n",
    "\n",
    "These lab exercises are marked, and contribute to your final grade. This lab exercise has 20 marks to earn, equivalent to 12% of your final grade.\n",
    "\n",
    "Please submit your completed workbook to the auto marker before 2021-11-14 20:00 GMT. The workbook you submit must be an .ipynb file, which is saved into the directory you're running Jupyter; alternatively you can download it from the menu above using `File -> Download As -> Notebook (.ipynb)`. Remember to save your work regularly (`Save and checkpoint` in the `File` menu, the icon of a floppy disk, or `Ctrl-S`). It is wise to verify it runs to completion with _Restart & Run All_ before submission.\n",
    "\n",
    "You must comply with the universities plagiarism guidelines: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification\n",
    "\n",
    "The system must provide a function ``search``, with the following specification:\n",
    "```\n",
    "def search(query, ordering = 'normal', count = 10):\n",
    "  ...\n",
    "```\n",
    "\n",
    "It `print`s out the results of the search, subject to the following rules:\n",
    "1. It selects from the set of all recipes that contain __all__ of the words in the query (the positions/order of the words in the recipe are to be ignored).\n",
    "2. It orders them based on the provided ordering (a string, meaning defined below).\n",
    "3. It `print`s the top `count` matches only, preserving the order from best to worst. Must `print` just their title, one per line. Must be less than `count` if the search is specific enough that less than `count` recipes match.\n",
    "\n",
    "As an aside, do not worry about memory usage. If duplicating some data can make your code faster/neater then feel free.\n",
    "\n",
    "\n",
    "\n",
    "### Data set\n",
    "\n",
    "A file, `recipes.json` is provided, containing 17K recipes. It can be parsed into a Python data structure using the [`json`](https://docs.python.org/3/library/json.html) module. It is a list, where each recipe is a dictionary containing various keys:\n",
    "* `title` : Name of recipe; you can assume these are unique\n",
    "* `categories` : A list of tags assigned to the recipe\n",
    "* `ingredients` : What is in it, as a list\n",
    "* `directions` : List of steps to make the recipe\n",
    "* `rating` : A rating, out of 5, of how good it is\n",
    "* `calories` : How many calories it has\n",
    "* `protein` : How much protein is in it\n",
    "* `fat` : How much fat is in it\n",
    "\n",
    "Note that the data set was obtained via web scrapping and hence is noisy - every key except for `title` is missing from at least one recipe. Your code will need to cope with this.\n",
    "\n",
    "You will probably want to explore the data before starting, so you have an idea of what your code has to deal with.\n",
    "\n",
    "Data set came from https://www.kaggle.com/hugodarwood/epirecipes/version/2 though note it has been cleaned it up, by deleting duplicates and removing the really dodgy entries.\n",
    "\n",
    "\n",
    "\n",
    "### Search\n",
    "\n",
    "The search should check the following parts of the recipe (see data set description below):\n",
    "* `title`\n",
    "* `categories`\n",
    "* `ingredients`\n",
    "* `directions`\n",
    "\n",
    "For instance, given the query \"banana cheese\" you would expect \"Banana Layer Cake with Cream Cheese Frosting\" in the results. Note that case is to be ignored (\"banana\" matches \"Banana\") and the words __do not__ have to be next to one another, in the same order as the search query or even in the same part of the recipe (\"cheese\" could appear in the title and \"banana\" in the ingredients). However, all words in the search query __must__ appear somewhere.\n",
    "\n",
    "\n",
    "\n",
    "### Tokenisation\n",
    "\n",
    "This is the term for breaking a sentence into each individual word (token). Traditionally done using regular expressions, and Python does have the `re` module, but there is no need to do that here (`re` can be quite fiddly). For matching words your tokenisation must follow the following steps:\n",
    "1. Convert all punctuation and digits into spaces. For punctuation use the set in [`string.punctuation`](https://docs.python.org/3/library/string.html#string.punctuation), for digits [`string.digits`](https://docs.python.org/3/library/string.html#string.digits). You may find [`translate()`](https://docs.python.org/3/library/stdtypes.html#str.translate) interesting!\n",
    "2. [`split()`](https://docs.python.org/3/library/stdtypes.html#str.split) to extract individual tokens.\n",
    "3. Ignore any token that is less than $3$ characters long.\n",
    "4. Make tokens lowercase.\n",
    "\n",
    "When matching words for search (above) or ordering (below) it's only a match if you match an entire token. There are many scenarios where this simple approach will fail, but it's good enough for this exercise. The auto marker will be checking the above is followed! When doing a search the code should ignore terms in the search string that fail the above requirements.\n",
    "\n",
    "\n",
    "\n",
    "### Ordering\n",
    "\n",
    "There are three ordering modes to select from, each indicated by passing a string to the `search` function:\n",
    "* `normal` - Based simply on the number of times the search terms appear in the recipe. A score is calculated and the order is highest to lowest. The score sums the following terms (repeated words are counted multiple times, i.e. \"cheese cheese cheese\" is $3$ matches to \"cheese\"):\n",
    "    * $8 \\times$ Number of times a query word appears in the title\n",
    "    * $4 \\times$ Number of times a query word appears in the categories\n",
    "    * $2 \\times$ Number of times a query word appears in the ingredients\n",
    "    * $1 \\times$ Number of times a query word appears in the directions\n",
    "    * The `rating` of the recipe (if not available assume $0$)\n",
    "\n",
    "* `simple` - Tries to minimise the complexity of the recipe, for someone who is in a rush. Orders to minimise the number of ingredients multiplied by the numbers of steps in the directions.\n",
    "\n",
    "* `healthy` - Order from lowest to highest by this cost function:\n",
    "$$\\frac{|\\texttt{calories} - 510n|}{510} + 2\\frac{|\\texttt{protein} - 18n|}{18} + 4\\frac{|\\texttt{fat} - 150n|}{150}$$\n",
    "Where $n \\in \\mathbb{N}^+$ is selected to minimise the cost ($n$ is a positive integer and $n=0$ is not allowed). This can be understood in terms of the numbers $510$, $18$ and $150$ being a third of the recommended daily intake (three meals per day) for an average person, and $n$ being the number of whole meals the person gets out of cooking/making the recipe. So this tries to select recipes that neatly divide into a set of meals that are the right amount to consume for a healthy, balanced diet. Try not to overthink the optimisation of $n$, as it's really quite simple to do!\n",
    "\n",
    "To clarify the use of the ordering string, to get something healthy that contains cheese you might call `search('cheese', 'healthy')`. In the case of a recipe that is missing a key in its dictionary the rules are different for each search mode:\n",
    "* `normal` - Consider a missing entry in the recipe (e.g. no `ingredients` are provided) to simply mean that entry can't match any search words (because it has none!), but the item is still eligible for inclusion in the results, assuming it can match the search with a different entry.\n",
    "* `simple` - If a recipe is missing either `ingredients` or `directions` it is dropped from such a search result. Because the data is messy if either of these lists is of length $1$ it should be assumed that the list extraction has failed and the recipe is to also be dropped from the search results.\n",
    "* `healthy` - If any of `calories`, `protein` or `fat` is missing the recipe should be dropped from the result.\n",
    "\n",
    "\n",
    "\n",
    "### Extra\n",
    "\n",
    "You may find the [_inverted index_](https://en.wikipedia.org/wiki/Inverted_index) interesting. It's a data structure used by search engines. For each word a user may search for this contains a list of all documents (recipes) that contain the word. This may take a little effort to understand, but the resulting code will be both faster and neater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advice\n",
    "\n",
    "* Don't just start coding: Make a plan and work out what you intend to do.\n",
    "* Think about structure, as messy code leads to mistakes.\n",
    "* Plan your data structures. Don't be afraid to use sheets of paper if that works for you!\n",
    "* Don't duplicate code, put it in a function/method instead.\n",
    "\n",
    "* Divide and conquer. Break the system into parts that can implemented with minimal dependency on the rest. Functions or OOP are both suitable for doing this.\n",
    "* Test early. Verify the individual parts work before trying to combine them. Factor this into the order you implement the parts of the system - don't implement something you are going to struggle to test before implementing, and verifying, other parts.\n",
    "* Do not try and do a 'big bang', where you get it all working at once. Instead, get it working with features missing, then add those features in, one at a time.\n",
    "\n",
    "* Keep things as simple as possible. Avoid long functions/methods.\n",
    "* Include comments, as a form of planning and for your own sanity!\n",
    "* Regularly reset the kernel and rerun the entire workbook. It is very easy to break something but not notice, because the correct version remains in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "* __20 marks__: For many different _unit tests_ that will be run on `search`. They will cover all of the details in the above specification!\n",
    "    * One test checks that it's faster than $0.1$ seconds on average (on the computer which runs the auto marker, which is quite fast) to do a search, so try not to be too inefficient (it ignores any time your notebook spends building data structures to be used by `search`). Note that the validation implementation comes in at $0.001$ seconds per search (after $5.5$ seconds of preparation), so this is very achievable!\n",
    "    * You may want to look into Python's `set()` object, as it is useful for _one_ of the possible ways to implement this.\n",
    "    * There will be sorting. The [Sorting how to](https://docs.python.org/3/howto/sorting.html) may help.\n",
    "    * The auto marker does give some feedback, and you can run it as many times as you want. Don't be afraid to test an incomplete or semi-broken version of your code if stuck or unsure. It may help!\n",
    "    * The validation implementation is 104 lines of code split over 5 cells (including white space for clarity and comments). Coded by someone who probably has much more experience than you, so you shouldn't aim to match this, but it's a good clue: If you find yourself at 500 lines you may want to stop and think some more! (line count does not include testing code, which is about the same amount again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import json\n",
    "import string\n",
    "import numpy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the file in f\n",
    "#F = open('/Users/souravpanda/Desktop/university of bath/study/software technology for data srecipes.json')\n",
    "data = json.load(open('recipes.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G = open('/Users/souravpanda/Desktop/university of bath/study/software technology for data science/lab/recipes.json')\n",
    "token_data = json.load(open('recipes.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_data = json.load(open('recipes.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_token_data = json.load(open('recipes.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s is a word, i is the char a is the new form of word\n",
    "def remove_punc(s):\n",
    "    a=''\n",
    "    lst = []\n",
    "    for i in s:\n",
    "        if i in string.punctuation:\n",
    "            a+=' '\n",
    "        elif i in string.digits:\n",
    "            pass\n",
    "        else:\n",
    "            a+=i.lower()\n",
    "    lst = a.split()\n",
    "    \n",
    "    for i in lst:\n",
    "        if len(i)<3:\n",
    "            lst.remove(i)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(token_data)):\n",
    "    token_data[i]['title']=remove_punc(token_data[i]['title'])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenising cont for rest\n",
    "tok_lst = ['categories','ingredients','directions']\n",
    "for i in range(len(token_data)):\n",
    "    for s in tok_lst:\n",
    "    \n",
    "        a= []\n",
    "        for j in token_data[i][s]:\n",
    "            a+=(remove_punc(j))\n",
    "        token_data[i][s]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(s_data)): \n",
    "    i = s_data[j] \n",
    "    if 'categories' not in i.keys(): \n",
    "        #print(j,'c')\n",
    "        i['categories'] = []\n",
    "    if 'ingredients' not in i.keys():\n",
    "        #print(j,'i')\n",
    "        i['ingredients'] = []\n",
    "    if 'directions' not in i.keys():\n",
    "        #print(j,'d')\n",
    "        i['directions'] = []\n",
    "    if 'rating' not in i.keys():\n",
    "        #print(j,'r')\n",
    "        i['rating'] = 0\n",
    "    if 'calories' not in i.keys():\n",
    "        #print(j,'d')\n",
    "        i['calories'] = 0\n",
    "    if 'protein' not in i.keys():\n",
    "        #print(j,'d')\n",
    "        i['protein'] = 0\n",
    "    if 'fat' not in i.keys():\n",
    "        #print(j,'d')\n",
    "        i['fat'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(token_data)):\n",
    "    token_data[i]['all_tok']= token_data[i]['title']+token_data[i]['categories']+token_data[i]['ingredients']+token_data[i]['directions']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(s_token_data)):\n",
    "    s_token_data[i]['all_tok']= s_token_data[i]['title']+s_token_data[i]['categories']+s_token_data[i]['ingredients']+s_token_data[i]['directions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(s_data)):\n",
    "    s_data[i]['hscore'] = abs((s_data[i]['calories']-510)/510)+abs((s_data[i]['protein']-18)/9)+abs((2/75)*(s_data[i]['fat']-150)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(s_token_data)):\n",
    "    #print(i)\n",
    "    s_data[i]['ilen'] = len(s_data[i]['ingredients'])\n",
    "    s_data[i]['dlen'] = len(s_data[i]['directions'])\n",
    "    s_data[i]['score'] = ( s_data[i]['ilen']*s_data[i]['dlen'])\n",
    "    s_token_data[i]['i'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_search(query,count):\n",
    "    s_result = []\n",
    "    s = query\n",
    "    for i in range(len(s_token_data)):\n",
    "        #print(set(s)<= set(token_data[0]['title']))\n",
    "        if (s_data[i]['ilen']==1 or s_data[i]['ilen']==0 or s_data[i]['dlen']==1 or s_data[i]['dlen']==0):\n",
    "            pass\n",
    "        else:\n",
    "        \n",
    "            if set(s) <= set(s_token_data[i]['all_tok']):\n",
    "                s_result.append(s_data[i])\n",
    "    out1 = sorted(s_result, key=lambda i: i['score'],)[:count]\n",
    "    return out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def healthy_search(query,count): \n",
    "    h_result = []\n",
    "    s = query\n",
    "    for i in range(len(s_token_data)):\n",
    "        if (s_data[i]['calories']==0 or s_data[i]['protein']==0 or s_data[i]['fat']==0):\n",
    "            #print('hi')\n",
    "            pass\n",
    "        else:\n",
    "            #print('j')\n",
    "            if set(s) <= set(s_token_data[i]['all_tok']):\n",
    "                #print('k')\n",
    "                h_result.append(s_data[i])\n",
    "\n",
    "    out = sorted(h_result, key=lambda i: i['hscore'],)[:count]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the miising key also giving them apropiate values.\n",
    "for j in range(len(token_data)): \n",
    "    i = token_data[j] \n",
    "    if 'categories' not in i.keys(): \n",
    "        #print(j,'c')\n",
    "        i['categories'] = ['Dose_not_eXist']\n",
    "    if 'ingredients' not in i.keys():\n",
    "        #print(j,'i')\n",
    "        i['ingredients'] = ['Dose_not_eXist']\n",
    "    if 'directions' not in i.keys():\n",
    "        #print(j,'d')\n",
    "        i['directions'] = ['Dose_not_eXist']\n",
    "    if 'rating' not in i.keys():\n",
    "        #print(j,'r')\n",
    "        i['rating'] = -1\n",
    "    if 'calories' not in i.keys():\n",
    "        #print(j,'d')\n",
    "        i['calories'] = -1\n",
    "    if 'protein' not in i.keys():\n",
    "        #print(j,'d')\n",
    "        i['protein'] = -1\n",
    "    if 'fat' not in i.keys():\n",
    "        #print(j,'d')\n",
    "        i['fat'] = -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenising the the title\n",
    "for i in range(len(token_data)):\n",
    "    token_data[i]['title']=remove_punc(token_data[i]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenising cont for rest\n",
    "tok_lst = ['categories','ingredients','directions']\n",
    "for i in range(len(token_data)):\n",
    "    for s in tok_lst:\n",
    "    \n",
    "        a= []\n",
    "        for j in token_data[i][s]:\n",
    "            a+=(remove_punc(j))\n",
    "        token_data[i][s]=a\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['blanketed', 'eggplant'],\n",
       " 'categories': ['tomato',\n",
       "  'vegetable',\n",
       "  'appetizer',\n",
       "  'side',\n",
       "  'vegetarian',\n",
       "  'eggplant',\n",
       "  'pan',\n",
       "  'fry',\n",
       "  'vegan',\n",
       "  'bon',\n",
       "  'appétit'],\n",
       " 'ingredients': ['small',\n",
       "  'japanese',\n",
       "  'eggplants',\n",
       "  'peeled',\n",
       "  'large',\n",
       "  'fresh',\n",
       "  'mint',\n",
       "  'leaves',\n",
       "  'large',\n",
       "  'garlic',\n",
       "  'cloves',\n",
       "  'slivered',\n",
       "  'flattened',\n",
       "  'cups',\n",
       "  'olive',\n",
       "  'oil',\n",
       "  'for',\n",
       "  'deep',\n",
       "  'frying',\n",
       "  'pounds',\n",
       "  'tomatoes',\n",
       "  'tablespoons',\n",
       "  'extra',\n",
       "  'virgin',\n",
       "  'olive',\n",
       "  'oil',\n",
       "  'medium',\n",
       "  'onion',\n",
       "  'chopped',\n",
       "  'fresh',\n",
       "  'basil',\n",
       "  'leaves',\n",
       "  'tablespoon',\n",
       "  'dried',\n",
       "  'oregano',\n",
       "  'tablespoons',\n",
       "  'drained',\n",
       "  'capers'],\n",
       " 'directions': ['place',\n",
       "  'eggplants',\n",
       "  'double',\n",
       "  'thickness',\n",
       "  'paper',\n",
       "  'towels',\n",
       "  'salt',\n",
       "  'generously',\n",
       "  'let',\n",
       "  'stand',\n",
       "  'hour',\n",
       "  'pat',\n",
       "  'dry',\n",
       "  'with',\n",
       "  'paper',\n",
       "  'towels',\n",
       "  'cut',\n",
       "  'deep',\n",
       "  'incisions',\n",
       "  'each',\n",
       "  'eggplant',\n",
       "  'using',\n",
       "  'tip',\n",
       "  'knife',\n",
       "  'push',\n",
       "  'mint',\n",
       "  'leaf',\n",
       "  'and',\n",
       "  'garlic',\n",
       "  'sliver',\n",
       "  'into',\n",
       "  'each',\n",
       "  'incision',\n",
       "  'pour',\n",
       "  'cups',\n",
       "  'oil',\n",
       "  'into',\n",
       "  'heavy',\n",
       "  'medium',\n",
       "  'saucepan',\n",
       "  'and',\n",
       "  'heat',\n",
       "  '°f',\n",
       "  'add',\n",
       "  'eggplants',\n",
       "  'batches',\n",
       "  'and',\n",
       "  'fry',\n",
       "  'until',\n",
       "  'deep',\n",
       "  'golden',\n",
       "  'brown',\n",
       "  'turning',\n",
       "  'occasionally',\n",
       "  'about',\n",
       "  'minutes',\n",
       "  'transfer',\n",
       "  'eggplants',\n",
       "  'paper',\n",
       "  'towels',\n",
       "  'and',\n",
       "  'drain',\n",
       "  'blanch',\n",
       "  'tomatoes',\n",
       "  'pot',\n",
       "  'boiling',\n",
       "  'water',\n",
       "  'for',\n",
       "  'seconds',\n",
       "  'drain',\n",
       "  'peel',\n",
       "  'tomatoes',\n",
       "  'cut',\n",
       "  'tomatoes',\n",
       "  'half',\n",
       "  'squeeze',\n",
       "  'out',\n",
       "  'seeds',\n",
       "  'chop',\n",
       "  'tomatoes',\n",
       "  'set',\n",
       "  'aside',\n",
       "  'heat',\n",
       "  'tablespoons',\n",
       "  'extra',\n",
       "  'virgin',\n",
       "  'olive',\n",
       "  'oil',\n",
       "  'large',\n",
       "  'pot',\n",
       "  'over',\n",
       "  'high',\n",
       "  'heat',\n",
       "  'add',\n",
       "  'flattened',\n",
       "  'garlic',\n",
       "  'cloves',\n",
       "  'sauté',\n",
       "  'until',\n",
       "  'light',\n",
       "  'brown',\n",
       "  'about',\n",
       "  'minutes',\n",
       "  'discard',\n",
       "  'garlic',\n",
       "  'add',\n",
       "  'onion',\n",
       "  'sauté',\n",
       "  'until',\n",
       "  'translucent',\n",
       "  'about',\n",
       "  'minutes',\n",
       "  'add',\n",
       "  'reduced',\n",
       "  'cups',\n",
       "  'stirring',\n",
       "  'occasionally',\n",
       "  'about',\n",
       "  'minutes',\n",
       "  'mix',\n",
       "  'capers',\n",
       "  'and',\n",
       "  'tablespoons',\n",
       "  'extra',\n",
       "  'virgin',\n",
       "  'olive',\n",
       "  'oil',\n",
       "  'into',\n",
       "  'sauce',\n",
       "  'season',\n",
       "  'with',\n",
       "  'salt',\n",
       "  'and',\n",
       "  'pepper',\n",
       "  'reduce',\n",
       "  'heat',\n",
       "  'add',\n",
       "  'eggplants',\n",
       "  'simmer',\n",
       "  'minutes',\n",
       "  'spooning',\n",
       "  'sauce',\n",
       "  'over',\n",
       "  'eggplants',\n",
       "  'occasionally',\n",
       "  'spoon',\n",
       "  'sauce',\n",
       "  'onto',\n",
       "  'platter',\n",
       "  'top',\n",
       "  'with',\n",
       "  'eggplants',\n",
       "  'serve',\n",
       "  'warm',\n",
       "  'at',\n",
       "  'room',\n",
       "  'temperature'],\n",
       " 'rating': 3.75,\n",
       " 'calories': 1386.0,\n",
       " 'protein': 9.0,\n",
       " 'fat': 133.0}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keys = [*data[1].keys()][:4]\n",
    "cal_keys = [*data[1].keys()][4:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = []\n",
    "for i in range(len(data)):\n",
    "    score = {}\n",
    "    for j in data[i]:\n",
    "        if j == search_keys[0]:\n",
    "            for w in token_data[i][j]:\n",
    "                if w in score:\n",
    "                    score[w]+=8\n",
    "                else:\n",
    "                    score[w]=8\n",
    "        if j == search_keys[1]:\n",
    "            for w in token_data[i][j]:\n",
    "                if w in score:\n",
    "                    score[w]+=4\n",
    "                else:\n",
    "                    score[w]=4\n",
    "        if j == search_keys[2]:\n",
    "            for w in token_data[i][j]:\n",
    "                if w in score:\n",
    "                    score[w]+=2\n",
    "                else:\n",
    "                    score[w]=2\n",
    "        if j == search_keys[3]:\n",
    "            for w in token_data[i][j]:\n",
    "                if w in score:\n",
    "                    score[w]+=1\n",
    "                else:\n",
    "                    score[w]=1\n",
    "        else:\n",
    "            pass\n",
    "    score['index'] = i\n",
    "    ii.append(score)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adult': 8,\n",
       " 'pimiento': 8,\n",
       " 'cheese': 13,\n",
       " 'vegetable': 4,\n",
       " 'cook': 4,\n",
       " 'vegetarian': 4,\n",
       " 'quick': 4,\n",
       " 'easy': 4,\n",
       " 'cheddar': 7,\n",
       " 'hot': 4,\n",
       " 'pepper': 5,\n",
       " 'winter': 4,\n",
       " 'gourmet': 4,\n",
       " 'alabama': 4,\n",
       " 'large': 3,\n",
       " 'garlic': 4,\n",
       " 'cloves': 2,\n",
       " 'ounce': 2,\n",
       " 'jar': 3,\n",
       " 'diced': 2,\n",
       " 'pimientos': 3,\n",
       " 'cups': 2,\n",
       " 'coarsely': 2,\n",
       " 'grated': 2,\n",
       " 'sharp': 2,\n",
       " 'preferably': 2,\n",
       " 'english': 2,\n",
       " 'canadian': 2,\n",
       " 'vermont': 2,\n",
       " 'about': 2,\n",
       " 'ounces': 2,\n",
       " 'cup': 2,\n",
       " 'mayonnaise': 3,\n",
       " 'crackers': 2,\n",
       " 'toasted': 2,\n",
       " 'baguette': 2,\n",
       " 'slices': 2,\n",
       " 'crudités': 2,\n",
       " 'force': 1,\n",
       " 'through': 1,\n",
       " 'press': 1,\n",
       " 'into': 1,\n",
       " 'bowl': 1,\n",
       " 'and': 4,\n",
       " 'stir': 2,\n",
       " 'with': 3,\n",
       " 'liquid': 1,\n",
       " 'add': 1,\n",
       " 'toss': 1,\n",
       " 'mixture': 1,\n",
       " 'combine': 1,\n",
       " 'well': 1,\n",
       " 'taste': 1,\n",
       " 'season': 1,\n",
       " 'freshly': 1,\n",
       " 'ground': 1,\n",
       " 'black': 1,\n",
       " 'spread': 3,\n",
       " 'may': 1,\n",
       " 'made': 1,\n",
       " 'day': 1,\n",
       " 'ahead': 1,\n",
       " 'chilled': 1,\n",
       " 'covered': 1,\n",
       " 'bring': 1,\n",
       " 'room': 1,\n",
       " 'temperature': 1,\n",
       " 'before': 1,\n",
       " 'serving': 1,\n",
       " 'serve': 1,\n",
       " 'accompaniments': 1,\n",
       " 'index': 0}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ii[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rating', 'calories', 'protein', 'fat']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query,ordering,count):\n",
    "    s = remove_punc(query)\n",
    "    \n",
    "    if ordering.lower() =='normal':\n",
    "        \n",
    "        indx=[]\n",
    "        result =[]\n",
    "        #lst_key = ['title', 'categories', 'ingredients', 'directions']\n",
    "        for i in range(len(token_data)):\n",
    "            #print(set(s)<= set(token_data[0]['title']))\n",
    "            if set(s) <= set(token_data[i]['all_tok']):\n",
    "                indx.append(i)\n",
    "                \n",
    "\n",
    "    #Score ------------------------------------------------------------    \n",
    "        score_lst = []\n",
    "        for i in range(len(result)):\n",
    "            score=0\n",
    "            for w in s:\n",
    "                for key in lst_key:\n",
    "                    if w in result[i]['score'][key]:\n",
    "                        #print(w,key,result[i]['score'][key][w])\n",
    "                        score +=result[i]['score'][key][w]\n",
    "            score+=result[i]['rating']\n",
    "            score_lst.append(score)\n",
    "    #top count no of score\n",
    "        indx_score = sorted(range(len(score_lst)), key=lambda i: score_lst[i], reverse = True)[:count]\n",
    "\n",
    "    #printing the results\n",
    "        for f in indx_score:\n",
    "            print(data[indx[f]]['title'])\n",
    "\n",
    "    #SEARCH SIMPLE---------------------------------------------\n",
    "    if ordering.lower() =='simple':\n",
    "        \n",
    "        si = simple_search(s,count)\n",
    "        for i in si:\n",
    "            print(i['title'])\n",
    "     #SEARCH HEALTHY---------------------------------------------\n",
    "    if ordering.lower()=='healthy':\n",
    "        \n",
    "        h=healthy_search(s,count)\n",
    "        for i in h:\n",
    "            print(i['title'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'all_tok'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-283d967e1389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bread'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'normal'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-62f2eee7cbe2>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(query, ordering, count)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m#print(set(s)<= set(token_data[0]['title']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all_tok'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mindx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'all_tok'"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "search('bread','normal',10)\n",
    "b=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27873682975769043"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
